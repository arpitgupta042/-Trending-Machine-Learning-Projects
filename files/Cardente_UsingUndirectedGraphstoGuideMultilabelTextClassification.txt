Using Undirected Graphs to Guide Mutli-label Text Classification
John Cardente 12/13/2013

1

Introduction

use of directed acyclic and edge-based graphs to perform multi-label classification.

The importance of multi-label classification has increased with the growth of online collaboration foNGNB rums containing large amounts of text on diverse top- 3 ics. User generated annotations, called tags [1], are commonly used to help discover relevant information. Many multi-label prediction algorithms, in particular These tags are often inconsistently applied which re- mixture models, explicitly consider tag co-occurrence duces their effectiveness. relationships during training but not prediction. The conditional probabilities generated by these algoSupervised machine learning techniques can be used rithms inherently confound the relationships between to identify the topics within a body of text and pretags. While these techniques are effective, improved dict the appropriate tags. Using these techniques accuracy and efficiency may be obtained through exmay substantially improve the consistency of text anplicitly considering tag relationships during predicnotation and make finding relevant information eastion. The Network Guided Naive Bayes (NGNB) ier. This requires machine learning algorithms capamodel presented in this paper explores this potential. ble of accurately modeling many tags and processing large data sets in reasonable amounts of time. During NGNB training, a binary Multinomial Naive Bayes classifier is learned for each tag. An undirected This paper presents and evaluates a new Network graph is also created from the tag co-occurrences in Guided Naive Bayes (NGNB) classifier that uses the training examples. Each graph node represents a undirected graphs to accurately and quickly predict single tag. An edge in the graph indicates that the labels for multi-topic text. The NGNB classifier two associated tags appeared together in a training is compared to Binary Relevance Multinomial and sample label. Self-loops are included in the graph to Parametric Mixture Naive Bayes models to determine represent the case when a tag appears alone. Counts its relative effectiveness. for each edge are recorded to represent the strength of the relationships. During prediction, NGNB first identifies the most likely tag using the per-tag Multinomial Naive Bayes models. This step ignores any tag relationships and Multi-label text classification is a well established evaluates each in isolation. For very large datasets, a field. Tsoumakas et al [4] and Puurala [5] provide graph search starting from highly central nodes can a good overview. Madjarov et al [6] compare various be used to find the most likely tag without having to multi-label prediction algorithms using a variety of evaluate all the tags. Once identified, the log-odds data sets. McCallum [7] and Ueda et al [8] present of the most likely node is scaled by the ratio of the mixture models based on Naive Bayes classifiers that self-loop edge count to the number of times the tag attempt to learn the correlations between tags. Wang appeared in the training set. The resulting weighted et al [9] and Zhang et al [10] respectively describe the log-odds is used to estimate the likelihood that the most likely tag should be predicted alone.

2

Related Work

 john.cardente@emc.com

Next, the undirected graph is used to determine the set of potentially relevant additional tags. Unlikely 1

tags are removed from this set using the per-tag Multinomial Naive Bayes predictions computed in the first step. The power-set of the remaining nodes, up to a configurable size limit, is computed. Each powerset combination is evaluated by inducing a sub-graph for the associated tags and propagating the per-tag log-odds values from the edge nodes to the node representing the most likely tag. A scaling factor, based on the smallest edge count in the graph, is used to reduce the contribution of the propagated log-odd values as they flow through the network. Using the smallest edge count introduces a penalty for large tag sets and prevents their over prediction. After the propagation phase is completed, the accumulated log-odds value at the node for the most likely tag is used to estimate the likelihood of the combination. After all the power-set combinations are evaluated, the set with the highest accumulated log-odds value is chosen as the final prediction.

                                                                                                                 





Figure 1: Tag co-occurrence undirected graph for training set.

4
4.1

Evaluation
Data

set to evaluate the effect of noisy data.

4.2
To evaluate the NGNB approach, a corpus of posts to the Stack Exchange website across a wide variety of topics [2] was utilized. The data set, provided by StackExchange, consisted of 6 million posts each containing a title, body, and one or more human generated labels representing the associated topics. The full data set contained 41928 unique tags with an average of 1.6 tags per post.

Models

To evaluate NGNB, two baseline models were implemented for comparison. A Binary Relevance Multinomial Naive Bayes (BRNB) model was used to establish the effectiveness of predicting each tag in isolation. In this model, a separate Multinomial Naive Bayes binary classifier was built for each tag. During prediction, a tag was selected if the positive outcome probability was greater than that of the negative outTo reduce the time and resources required to evalucome. No size limit was placed on the final predicted ate the NGNB and other models, a smaller training set of tags. data set was created from 200000 posts containing 116 commonly occurring tags. Figure 1 illustrates A Parametric Mixture Model (PMM1) was used the tag co-occurrence undirected graph for this train- to represent the effectiveness of considering tag ocing set. The network consists of a single connected currence relationships while calculating the wordcomponent containing 176 edges between the 116 tag tag conditional probabilities. During training, the nodes. The NGNB approach can also be applied to PMM1 algorithm uses the tags associated with each networks containing multiple connected components. sample to adjust the conditional word probabilities. After applying an Expectation Maximization process, To further reduce the evaluation time and resources, the resulting conditional word-tag probabilities imuninformative words were removed from the training plicitly reflect the influence of the tag co-occurrence set using within-class popularity and Gini Coefficient relationships. During prediction, a greedy approach metrics based on [3]. is used to iteratively select the set of tags that most An independent set of 15000 posts were selected to increase the likelihood until it cannot be increased create a test data set. The uninformative words iden- further. See [8] for further details of the PMM1 algotified in the training were not removed from the test rithm. 2

The NGNB implementation follows the description provided in Section 3. All of the models were implemented from scratch in the Python programming language. In all three cases, separate classifiers were built for the title and body features of the sampled posts. The union of the tags predicted by the title and body classifiers were used for the overall prediction.

Train 200 150
q

Test
q

q

100 50
q q

q

brnb

pmm1

ngnb

brnb

pmm1

ngnb

4.3

Cross Validation

Figure 3: Cross validation per-fold train and test execution times. Values are in seconds, dots represent the mean, and error bars reflect the 95% confidence interval.

Figure 2 provides the recall, precision, and F1 score results from using 10-fold cross validation to evaluated the three models. The BRNB model achieved the highest recall but lowest precision yielding the overall lowest F1 score. Both PMM1 and NGNB performed better with NGNB achieving the best results across all three measures. These results indicate that the naive approach of ignoring tag dependencies substantially reduces performance. They also show that considering these dependencies during the testing phase (NGNB) can be as effective as in the training phase (PMM1).
Recall
q

Although the performance of all three models decrease as the number of tags increase, NGNB exhibits the slowest rate of decay. Figure 5 illustrates the average per-fold training times for the models. The data shows that the PMM1 model's training time dramatically increases with the number of tags. BRNB and NGNB similarly exhibited a substantially smaller increase in training time as the number of tags increased suggesting that these algorithms are more scalable.

Precision

F1 F1

0.8 0.6
q q q q q q q
q q

0.8

q q q q q q q q q

0.4 0.6 0.2 brnb pmm1 ngnb
q

brnb pmm1 ngnb

brnb pmm1 ngnb

0.4 10 20 30 40

q

50

Figure 2: Recall, Precision, and F1 score results from 10-fold cross validation testing.

brnb

pmm1

ngnb

Figure 3 illustrates the average per-fold train and test execution times for the models. The data shows that the PMM1 model required substantially more time to train than the simpler BRNB and NGNB models. PMM1 also took longer to classify the test folds. In the both the training and testing phases, NGNB performed similarly to BRNB.

Figure 4: F1 score 10-fold cross validation results for increasing number of tags.

4.5

Test Set

Figure 6 provides the recall, predicision, and F1 results from classifying the test data set using the three models. As in the case of cross validation, NGNB proFigure 4 provides the F1 score results for the three vided the best results indicating that the algorithm models while increasing the number of possible tags. generalizes well even in the presence of noisy samples.

4.4

Tag Scaling

3

Train 80 60 40 20 0
q q q q q

may be worthy of further investigation and development to enable multi-label classification of large online datasets.

q q

q

References
[1] http://en.wikipedia.org/wiki/Tag_(metadata)

10

20

30

40

50

brnb

pmm1

ngnb

[2] http://blog.stackoverflow.com/2009/06/stackoverflow-creative-commons-data-dump/
Figure 5: Average cross validation perfold train execution times. Y axis is time in seconds. X axis is the number of tags.

[3] S. Singh, H. Murthy, and T. Gonsalves. Feature selection for text classification based on gini coefficient of inequality. Journal of Machine Learning Research-Proceedings Track 10, 76-85. [4] G. Tsoumakas, and I. Katakis. Multi-label classification: An overview. International Journal of Data Warehousing and Mining (IJDWM), 2007, 3(3), 1-13. [5] A. Puurula. Mixture Models for Mutli-label Text Classification University of Waikato. [6] G. Madjarov, D. Kocev, D. Gjorgjevikj, and S. Dzeroski. An extensive experimental comparison of methods for multi-label learning. Pattern Recognition 45.9 2012, 3084-3104. [7] A. McCallum. Multi-label text classification with a mixture model trained by EM. AAAI'99 Workshop on Text Learning. 1999,1-7. [8] N. Ueda, K. Saito. Parametric mixture models for multi-labeled text. Advances in neural information processing systems. 2002, 721-728. [9] X. Wang, and G. Sukthankar. Multi-label relational neighbor classification using social context features. Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining ACM, 2013, 464-472. [10] M. Zhang and K. Zhang. Multi-label learning by exploiting label dependency. Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining ACM, New York, NY, USA, 999-1008. [11] C. Manning, P. Raghavan, and H. Schutze. Introduction to information retrieval. Cambridge University Press 2008

Recall
q

Precision

F1

0.8 0.6
q q q q q q q

0.4 0.2 brnb pmm1 ngnb
q

brnb pmm1 ngnb

brnb pmm1 ngnb

Figure 6: Recall, Precision, and F1 score results from classifying the test data set.

5

Conclusions

The growth of online collaborative forums has made multi-label classification a common activity. The size and diversity of these data sets create the need for scalable machine learning algorithms capable of modeling many labels with varying degrees of dependency. Mixture models are a common method used in multilabel classification. This paper presented data indicating that such models may require substantial training time to calculate the conditional probabilities representing the tag relationships. To overcome this challenge, this paper presented a new algorithm call Network Guided Naive Bayes (NGNB) that deferred consideration of the tag dependencies to the prediction phase. Data from cross validation, tag scaling, and test data-set testing provided evidence that NGNB can be as effective as complex mixture models such as PMM1 while only requiring training and test times comparable to simpler models like BRNB. NGNB also degraded the least as the tag population size increased suggesting better scalability. These findings suggest that the NGNB method 4

